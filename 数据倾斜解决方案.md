## 数据倾斜

* **Hadoop中的数据倾斜现象**

  Hive的数据倾斜，一般都发生在 Sql 中 **Group 和 On** 上，而且和数据逻辑绑定比较深，详细的看日志或者和监控界面的话会发现：
   * 有一个或几个 reduce 卡住
   * 各种 container 报错 OOM
   * 读写的数据量极大，至少远远超过其它正常的 reduce
   * 伴随着数据倾斜，会出现任务被 kill 等各种诡异的表现。
  


* **问题发现与定位**

   * **通过 key 统计**

      由于数据量巨大，可以采用抽样的方式，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个，Spark中可以这么操作：

```
df.select("key").sample(false, 0.1)       // 数据采样
.(k => (k, 1)).reduceBykey(_ + _)         // 统计 key 出现的次数
.map(k => (k._2, k._1))                   // 根据 key 出现次数进行排序
.sortByKey(false).take(10)                // 取前 10 个
```

* **如何缓解数据倾斜**

   * **基本思路**

      * **从业务逻辑解决**: 
      
          我们从业务逻辑的层面上来优化数据倾斜，比如要统计不同城市的订单情况，那么我们单独对这一线城市来做 count，最后和其它城市做整合。
      
      * **从程序参数调优**: 
      
          Hadoop 和 Spark 都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。
          
         * set hive.map.aggr=true，在map中会做部分聚集操作，效率更高但需要更多的内存
         * set hive.exec.parallel.thread.number=8;设置task的并行度
         * hive.groupby.skewindata=true: 有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作
         * set mapreduce.map.output.compress=true;设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读写和网络传输，能提高很多效率）
         
           set hive.exec.compress.intermediate=true;开启hive中间传输数据压缩功能
         
           set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec;设置mapreduce中map输出数据的压缩方式
      * **从程序解决**
         * **场景一**：Reduce端Join转化为Map端Join
      
             通常用于一个很小的表和一个大表进行join的场景
        
             具体小表有多小，hive中由参数hive.mapjoin.smalltable.filesize来决定，默认25M, 0.7版本之后，hive默认开启mapjoin，由参数hive.auto.convert.join=true控制
        
         * **场景二**：拆分join再union
      
             适用于两张表都比较大，无法使用Map端Join，其中一个表有少数几个Key的数据量过大，另外一个表的Key分布较为均匀。
        
             将有数据倾斜的表中倾斜Key对应的数据加上随机前缀，另外一个表每条数据分别与随机前缀结合（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。
     
        * **场景三**：大表key加盐，小表扩大N倍join 
     
            如果倾斜 Key非常多，则另一侧数据膨胀非常大，而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销，场景二方案不适用。
            该方法其实就是上一个方法的特例或者简化，不拆分，也就没有union
            
        * **场景四**：Map端先局部聚合 
        
            使用reduceByKey而不是groupByKey
            
            在map 端加个combiner 函数进行局部聚合。加上 combiner 相当于提前进行 reduce ,就会把一个 mapper 中的相同 key 进行聚合，减少 shuffle 过程中数据量 以及 reduce 端的计算量。这种方法可以有效的缓解数据倾斜问题，但是如果导致数据倾斜的 key 大量分布在不同的 mapper 的时候，这种方法就不是很有效了。
            
        * **场景五**：加盐局部聚合 + 去盐全局聚合
        
            这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个 key 都打上一个 1~n 的随机数，比如 3 以内的随机数，此时原先一样的 key 就变成不一样的了，比如 (hello, 1) (hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成 (1_hello, 1) (3_hello, 1) (2_hello, 1) (1_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reduceByKey 等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了 (1_hello, 2) (2_hello, 2) (3_hello, 1)。然后将各个 key 的前缀给去掉，就会变成 (hello, 2) (hello, 2) (hello, 1)，再次进行全局聚合操作，就可以得到最终结果了，比如 (hello, 5)
        
