## 数据倾斜

* **Hadoop中的数据倾斜现象**

  Hive的数据倾斜，一般都发生在 Sql 中 **Group 和 On** 上，而且和数据逻辑绑定比较深，详细的看日志或者和监控界面的话会发现：
   * 有一个或几个 reduce 卡住
   * 各种 container 报错 OOM
   * 读写的数据量极大，至少远远超过其它正常的 reduce
   * 伴随着数据倾斜，会出现任务被 kill 等各种诡异的表现。
  


* **问题发现与定位**

   * **通过 key 统计**

      由于数据量巨大，可以采用抽样的方式，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个，Spark中可以这么操作：

```
df.select("key").sample(false, 0.1)       // 数据采样
.(k => (k, 1)).reduceBykey(_ + _)         // 统计 key 出现的次数
.map(k => (k._2, k._1))                   // 根据 key 出现次数进行排序
.sortByKey(false).take(10)                // 取前 10 个
```

* **如何缓解数据倾斜**

   * **基本思路**

      * **从业务逻辑解决**: 我们从业务逻辑的层面上来优化数据倾斜，比如要统计不同城市的订单情况，那么我们单独对这一线城市来做 count，最后和其它城市做整合。
      
      * **从程序参数调优**: Hadoop 和 Spark 都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。
         * set hive.map.aggr=true，在map中会做部分聚集操作，效率更高但需要更多的内存
         * set hive.exec.parallel.thread.number=8;设置task的并行度
         * hive.groupby.skewindata=true: 有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作
         * set mapreduce.map.output.compress=true;设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读写和网络传输，能提高很多效率）
         
           set hive.exec.compress.intermediate=true;开启hive中间传输数据压缩功能
         
           set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec;设置mapreduce中map输出数据的压缩方式
   * **场景分析**
      * **场景一**：Reduce端Join转化为Map端Join
      
        通常用于一个很小的表和一个大表进行join的场景
        
        具体小表有多小，hive中由参数hive.mapjoin.smalltable.filesize来决定，默认25M, 0.7版本之后，hive默认开启mapjoin，由参数hive.auto.convert.join=true控制
        
      * **场景二**：拆分join再union
      
        适用于两张表都比较大，无法使用Map端Join，其中一个表有少数几个Key的数据量过大，另外一个表的Key分布较为均匀。
        
        将有数据倾斜的表中倾斜Key对应的数据加上随机前缀，另外一个表每条数据分别与随机前缀结合（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。
     
     * **场景三**：大表key加盐，小表扩大N倍jion 
     
        如果倾斜 Key非常多，则另一侧数据膨胀非常大，场景二方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销
        
