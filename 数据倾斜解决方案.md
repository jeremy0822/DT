## 数据倾斜
* 问题发现与定位

   * 通过 key 统计

      由于数据量巨大，可以采用抽样的方式，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个：

```
df.select("key").sample(false, 0.1)       // 数据采样
.(k => (k, 1)).reduceBykey(_ + _)         // 统计 key 出现的次数
.map(k => (k._2, k._1))                   // 根据 key 出现次数进行排序
.sortByKey(false).take(10)                // 取前 10 个
```

* 如何缓解数据倾斜

   * 基本思路

      * 业务逻辑: 我们从业务逻辑的层面上来优化数据倾斜，比如要统计不同城市的订单情况，那么我们单独对这一线城市来做 count，最后和其它城市做整合。
      * 程序实现: 比如说在 Hive 中，经常遇到 count（distinct）操作，这样会导致最终只有一个 reduce，我们可以先 group 再在外面包一层 count，就可以了；在 Spark 中使用 reduceByKey 替代 groupByKey 等。
      * 参数调优: Hadoop 和 Spark 都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。

   * 思路1：过滤异常数据

     如果导致数据倾斜的 key 是异常数据，那么简单的过滤掉就可以了。

     首先要对 key 进行分析，判断是哪些 key 造成数据倾斜。具体方法上面已经介绍过了，这里不赘述。

     然后对这些 key 对应的记录进行分析：

      * 空值或者异常值之类的，大多是这个原因引起
      * 无效数据，大量重复的测试数据或是对结果影响不大的有效数据
      * 有效数据，业务导致的正常数据分布

   * 解决方案

     对于第 1，2 种情况，直接对数据进行过滤即可。第3种情况则需要特殊的处理，具体我们下面详细介绍。
